[
  {
    "path": "roadmap_gallery/2024-03-15-plotting-temp/",
    "title": "Data visualization with Python",
    "description": "I provide `Python` code and examples for a wide variety of \ndata visualizations, using the standard [machine learning workflow]() \n(explore, describe, infer, model, evaluate, tune, etc.) as a roadmap \nand letting the code and visualizations do most of the talking.\nThe custom stylesheet and plotting routines I reference throughout \ncan be found in my [datavis_utils]() repository.",
    "author": [
      {
        "name": "Matthew Bain",
        "url": {}
      }
    ],
    "date": "2024-03-16",
    "categories": [
      "reference"
    ],
    "contents": "\n\nContents\n1 Global settings\n2 Colours\n3 Tools for data representation & manipulation\n4 Tools for matrix visualization\n5 Preliminary tests\n6 Grid\n7 Seaborn extensions for statistical plotting\n8 Additional seaborn plots\n9 Math\n10 Analysis\n11 Stats\n12 Classifier deep-dive\n13 ML\n14 Model comparison routines (dataset x model/hyperparameter)\n\n\n### Establish project paths\nroot = '/content/drive/MyDrive/Colab_Notebooks/'\nproject_dir = 'experiments/'\noutput_dir  = 'stylesheet_tests/'\n\nproject_path = root + project_dir\noutput_path = root + project_dir + output_dir\nenv_path = project_path + 'requirements.txt'\n\n1 Global settings\nVisual elements and parameters I consider when establishing defaults\n\n\nknitr::include_graphics(\"../../images/attribute_histogram_plots.png\")\n\n\n\nFigure 1: Pairplot of variables related to housing prices.\n\n\n\n\n#>      Name Age Score\n#> 1   Alice  25    85\n#> 2     Bob  30    92\n#> 3 Charlie  22    78\n\n\n\nknitr::kable(df, caption = \"Caption\", digits = 2)\n\nTable 1: Caption\nName\nAge\nScore\nAlice\n25\n85\nBob\n30\n92\nCharlie\n22\n78\n\nGeneric\ntext (x/y/title/tick labels/legend title/legend items): font face/size\naxes: which to display/linewidth/minMax tick sizes/minMax tick widths\ngrid: which minMax VerticalHorizontal lines to display/colour/linewidth\nobject: edgecolour/edgewidth\ncolours: discrete cycle/continuous map\nlegend: box/placement\nfigure: size\nsubplots: vertical padding/horizontal padding\nother: layout compactness, display quality\nAdditional specifications for particular objects\nscatter: marker size\nline: width/continuous error colour/continuous error linewidth\nbar: width/between group spacing/errorbar colour/errorbar linewidth/errorbar cap width\ngrouped bar: within group spacing\nbox: median colour/median width\nNote: for each of the above, despite generic object defaults, may additionally have to specify:\n- edgecolour/edgewidth\nParameters to consider manually specifying for each (sub)plot:\nlabels/title: on or off\ntick labels: rotation, which ticks\naxes: limits, which ticks\ngrid: which lines\nobject: alpha\nfigure: size\nsubplots: RowColPosition arrangement, (shared) labels/titles\nTypical plotting function arguments:\ndata, fill variable, size variable\nplot type, error type, kernel type, regression type\nbin/sampling resolution\nReferences\n- The matplotlibrc file - matplotlib.org\nNote\n- All lines in the matplotlibrc file start with a ‘#’, so that removing all leading ’#’s yields a valid style file\n\n### [SCRATCH] Get fonts\nfrom pathlib import Path\nfrom matplotlib import font_manager as fm\nfrom fontTools.ttLib import TTFont\n\n# !wget 'https://github.com/google/fonts/blob/main/ofl/allison/Allison-Regular.ttf'\nfont_file = Path(*fm.findSystemFonts('.'))\nfont_file_path = Path(font_file)\n# fm.fontManager.addfont(font_file_path) # [TODO] resolve this\n\n\n### [SCRATCH]\nimport os\nitem = 'Abel-Regular.ttf'\noverwrite = 1\n\nsource_path = os.path.join('/content/', item)\noutput_path = os.path.join(project_path, output_dir, item)\n\nprint(f\"Copying: {item} from {source_path} to {output_path}\")\n\nif os.path.isfile(source_path) or os.path.isdir(source_path):\n    # Check if the item already exists in the output path\n    if os.path.exists(output_path):\n        if overwrite:\n            # Copy the item\n            #!cp -rf \"$source_path\" \"$output_path\"\n            print(f\"Successfully copied {item} ->\\n{output_path}\\n\")\n        else:\n            print(f\"Skipped: {item} already exists in {output_path}\\n\")\n    else:\n        # Copy the item\n        #!cp -rf \"$source_path\" \"$output_path\"\n        print(f\"Successfully copied {item} to {output_path}\\n\")\n\n\n### [SCRATCH]\n# !pip install pipreqs\n# !pipreqs > requirements.txt --force\n\n\n### View path to base matplotlibrc file\nimport matplotlib\nmatplotlib.matplotlib_fname()\n\n\n### STYLESHEET TEMPLATE\n# font.family : 'sans'\n\n\n### Stylesheet. Contrast with https://matplotlib.org/stable/users/explain/customizing.html#the-matplotlibrc-file\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\n\n## General\n# Font face and sizes\nmpl.rcParams['font.family'] = 'sans-serif'\n# mpl.rcParams['font.sans-serif'] = \"Helvetica\"\nmpl.rcParams['font.size'] = 8             # default font sizes\nmpl.rcParams['axes.titlesize'] = 12       # large\nmpl.rcParams['axes.labelsize'] = 9        # medium\nmpl.rcParams['xtick.labelsize'] = 8       # medium\nmpl.rcParams['ytick.labelsize'] = 8       # medium\nmpl.rcParams['legend.fontsize'] = 9       # medium\nmpl.rcParams['legend.title_fontsize'] = 9 # None (same as default axes)\nmpl.rcParams['figure.titlesize'] = 15     # large (suptitle size)\nmpl.rcParams['figure.labelsize'] = 12     # large (sup[x|y]label size)\n\n\n# Spines and ticks\n# mpl.rcParams['axes.spines.top'] = False\n# mpl.rcParams['axes.spines.right'] = False\nmpl.rcParams['axes.linewidth'] = .6\nmpl.rcParams['axes.edgecolor'] = 'black'\nmpl.rcParams['xtick.major.size'] = 0 # 3.5\nmpl.rcParams['ytick.major.size'] = 0 # 3.5\n# mpl.rcParams['xtick.major.width'] =  0.8\n# mpl.rcParams['ytick.major.width'] =  0.8\n\n# Grid\nmpl.rcParams['axes.grid.which'] = 'major' # lines at {major, minor, both} ticks\nmpl.rcParams['grid.linestyle'] = '--'\nmpl.rcParams['grid.color'] = '#CCCCCC'\nmpl.rcParams['grid.linewidth'] = 0.2\n# mpl.rcParams['grid.alpha'] = 1\n\n# Label placement\nmpl.rcParams['axes.titlelocation'] = 'center' # {left, right, center}\nmpl.rcParams['axes.titlepad'] = 7.5 # 6\nmpl.rcParams['axes.labelpad'] = 7.5 # 4\n# mpl.rcParams['xtick.major.pad'] = 3.5 # distance to major tick label in points\n# mpl.rcParams['ytick.major.pad'] = 3.5\n\n# Discrete color cycle (and continuous map)\n# mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\nmpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=sns.color_palette(\"PiYG\", n_colors=6))\n\n# Legend properties\nmpl.rcParams['legend.loc'] = 'best'\nmpl.rcParams['legend.frameon'] = False\nmpl.rcParams['legend.loc'] = 'best'\n\n# Legend padding\n# mpl.rcParams['legend.borderpad'] =  0.4     # border whitespace\n# mpl.rcParams['legend.labelspacing'] = 0.5   # vert space between legend entries\n# mpl.rcParams['legend.handlelength'] = 2.0   # length of the legend lines\n# mpl.rcParams['legend.handleheight'] = 0.7   # height of the legend handle\n# mpl.rcParams['legend.handletextpad'] = 0.8  # space btwn legend line legend text\n# mpl.rcParams['legend.borderaxespad'] = 0.5  # border btwn axes and legend edge\n# mpl.rcParams['legend.columnspacing'] = 2.0  # column separation\n\n# Space-filling object properties (e.g., polygons/circles, scatter)\nmpl.rcParams['patch.edgecolor'] = 'blue' # if forced, else patch is not filled\nmpl.rcParams['patch.force_edgecolor'] = 1\nmpl.rcParams['patch.linewidth'] = 0 # edgewidth (.5)\n\n## Specific objects\n# Scatter properties\n# mpl.rcParams['scatter.edgecolors'] = 'black'\n\n# Line properties\nmpl.rcParams['lines.markersize'] = 5\nmpl.rcParams['lines.linewidth'] = 2\n\n# Bar properties\n# mpl.rcParams['bar.width'] = 0.8\n\n# Error properties\nmpl.rcParams['errorbar.capsize'] = 1\n# mpl.rcParams['errorbar.color'] = 'black'\n# mpl.rcParams['errorbar.linewidth'] = 1.5\n\n# Box properties\n# box\nmpl.rcParams['boxplot.boxprops.linewidth'] = 0 # box outline (0.5)\n# mpl.rcParams['boxplot.boxprops.color'] = 'none' # 'black' (?)\n\n# box line to cap\nmpl.rcParams['boxplot.whiskerprops.linewidth'] = .65\nmpl.rcParams['boxplot.whiskerprops.linestyle'] = '--'\n# mpl.rcParams['boxplot.whiskerprops.color'] = 'black' # (?)\n\n# box cap line\nmpl.rcParams['boxplot.capprops.linewidth'] = .75\n# mpl.rcParams['boxplot.capprops.color'] = 'black' # (?)\n\n# box median line\nmpl.rcParams['boxplot.medianprops.linewidth'] = 1\nmpl.rcParams['boxplot.medianprops.linestyle'] = '-'\n# mpl.rcParams['boxplot.medianprops.color'] = 'black' # (?)\n\nmpl.rcParams['boxplot.meanprops.linewidth'] = 1\nmpl.rcParams['boxplot.meanprops.linestyle'] = '-'\n# mpl.rcParams['boxplot.meanprops.color'] = 'black' # (?)\n\n# box scatter\nmpl.rcParams['boxplot.flierprops.markerfacecolor'] = 'none'\nmpl.rcParams['boxplot.flierprops.markeredgewidth'] = .65\nmpl.rcParams['boxplot.flierprops.marker'] = 'o'\n# mpl.rcParams['boxplot.flierprops.markersize'] = 6 # (?)\n# mpl.rcParams['boxplot.flierprops.linewidth'] = 0 # (?)\n# mpl.rcParams['boxplot.flierprops.markeredgecolor'] = 'black' # (?)\n# mpl.rcParams['boxplot.flierprops.color'] = 'black' # (?)\n\n## Figure padding\n# Figure layout\nmpl.rcParams['figure.autolayout'] = True # auto- make plot elements fit on fig\nmpl.rcParams['figure.constrained_layout.use'] = True # apply tight layout\n\n# Subplot padding (all dims are a fraction of the fig width and height).\n# Not compatible with constrained_layout.\n# mpl.rcParams['figure.subplot.left'] = .125   # left side of subplots of fig\n# mpl.rcParams['figure.subplot.right'] = 0.9    # right side of subplots of fig\n# mpl.rcParams['figure.subplot.bottom'] = 0.11  # bottom of subplots of fig\n# mpl.rcParams['figure.subplot.top'] = 0.88     # top of subplots of figure\n# mpl.rcParams['figure.subplot.wspace'] = 0.2   # w reserved space btwn subplots\n# mpl.rcParams['figure.subplot.hspace'] = 0.2   # h reserved space btwn subplots\n\n# Constrained layout padding. Not compatible with autolayout.\n# mpl.rcParams['figure.constrained_layout.h_pad'] = 0.04167\n# mpl.rcParams['figure.constrained_layout.w_pad'] = 0.04167\n\n# Constrained layout spacing between subplots, relative to the subplot sizes.\n# Much smaller than for tight_layout (figure.subplot.hspace, figure.subplot.wspace)\n# as constrained_layout already takes surrounding text (titles, labels, # ticklabels)\n# into account. Not compatible with autolayout.\n# mpl.rcParams['figure.constrained_layout.hspace'] = 0.02\n# mpl.rcParams['figure.constrained_layout.wspace'] = 0.02\n\n## Other\n# Figure size and quality\nmpl.rcParams['figure.dpi'] = 100 # [NOTE] alters figure size\nmpl.rcParams['figure.figsize'] = (5, 5) # (6, 4), (6.4, 4.8)\n\n# Figure saving settings\nmpl.rcParams['savefig.transparent'] = True\nmpl.rcParams['savefig.format'] = 'png'\nmpl.rcParams['savefig.dpi'] = 330\n\n#%config InlineBackend.figure_format = 'svg' # set inline figure format/quality\n\n2 Colours\n[TODO]\nColour palette generator utilities\n[TODO] [def] Continuous generators:\n(1 H nodes) Monchromatic (linear L)\n- args: 1 H node, optional n L steps from it\n- example: blues\n- utility: topological\n- use cases: surface/distribution/contour plot\n(~2-6 H nodes) Sequential (constant L):\n- args: 2-6 H nodes, optional n(k - 1) transition steps between them\n- example: viridis/plasma\n- utility: topological\n- use cases: surface/distribution/contour plot\n(2 H nodes) Diverging (triangular L):\n- args: 2 H nodes, optional n*2 ascending/descending L steps between them\n- example: PiYG/RdBu\n- utility: 1D bipolar scale/2D binary probability surface\n- use cases: heatmap, classifier decision surface\n(~3-6 H nodes) Sequential diverging (~triangular L)\n- args: k H nodes, optional n(k - 1)*2 ascending/descending L steps between them\n- example: ~gnuplot2/gist_ncar/gist_rainbow/jet\n- utility: 1D multipolar scale\n- use cases: heatmap with 1+ intermediate nodes\n/ (~6 H nodes) Spectral (~constant L, cyclic /H)\n- example: hsv\n- utility: multiclass probability surface (using class probability-weighted H nodes)\n- ues cases: multiclass classifier decision surface\n[TODO] [def] Discrete generators:\n(k H nodes) Discrete (~constant L)\n- args: k H nodes\n- example: tab10/Dark2\n- utility: discrete objects with no within-group levels OR groups of objects with within-group levels but no order\n- use cases: k objects (lines/hists/bars/boxplots) for k categories OR m objects (within groups) for m levels\n(k H nodes) Discrete monochromatic (~constant L between * linear L within)\n- args: k H nodes, optional n*k L steps from each node\n- example: tab20/paired > tab20c/tab20b\n- utility: discrete groups of objects with a within-group order and aim to highlight between-group differences\n- use cases: (group * time period) * value grouped objects (lines/hists/bars/boxplots)\nColour generator utillity\nGiven (a) any H value and k-iary scheme / (b) H/set of H values and integer k:\n- (a) [def] return complimentary/triad/square/k-iary H nodes\n- (b) [def] return k quantized H nodes\n[TODO] For all of the above:\n- args: accept k colours and optional n steps\n- convert inputs to H values [0-359]\n- (maximize S/L of input args)\n- if multiple H nodes passed, quantize so each evenly spaced on colorwheel\n- return: k H nodes (+ either colourmap or discrete colourset)\n- plot: H nodes on colourwheel (+ colourbar)\n[TODO] Other:\n- demo how to convert any map hsl values\n[REF]\nTools\n- Adobe Colorwheel\nMatplotlib\n- Choosing Colormaps\n- Creating Colormaps\nOther libraries\n- seaborn\n- CMasher\n- Colormaps\n- Palettable\nTheory\n- Standard color space: HSL and HSV - Wikipedia\n- Perceptually uniform color space: HCL - hclwizard\n- Human-friendly HSL! : HSLuv - HSLuv.org\nOther\n- Reference of good perceptual palettes: HCL-Based Color Palettes - colorspace\n\n#!pip install cmasher\n\n\n### Continuous maps\n# (1) Monochromatic (1 H, linear L)\n#import cmasher as cmr\n#cmr.get_sub_cmap('Blues_r', 0, 1, N=20)\n\n\n#cmr.get_sub_cmap('Greens_r', 0, 1, N=20)\n\n\n# (2) Sequential (multiple H, constant L)\n#cmr.get_sub_cmap('plasma', 0, 1, N=20)\n\n\n#cmr.get_sub_cmap('cmr.chroma', 0.2, .8, N=20)\n\n\n### Constructing monochromatic (rot=.1)/sequential/cyclic (rot=1) maps w/ linear L\n# starting H [0, 3] (ROYGBIV = 3/7*h), rotations around hue wheel (int)\n# dark/light (intensity of darkest/lightest color in palette) [0, 1] or gamma\nListedColormap(sns.cubehelix_palette(start=(3/7)*1, rot=.1, reverse=True, gamma=.7,\n                                     # dark=.25, light=.85,\n                                     n_colors=20, as_cmap=False))\n\n\n# (3) Diverging (~2 H, triangular L)\n#cmr.get_sub_cmap('PiYG', 0, 1, N=20)\n\n\n#cmr.get_sub_cmap('Spectral', 0, 1, N=20)\n\n\n# (4) Sequentially diverging (3-6 H, ~triangular L)\n#cmr.get_sub_cmap('gnuplot', 0, 1, N=20)\n\n\n### Constructing diverging/sequentially diverging map\n# starting/ending H [0, 359], S [0, 100], L [0, 100]\nn_colors = 50\npalette_1 = sns.diverging_palette(h_neg=100, h_pos=200, s=100, l=50, n=n_colors, as_cmap=False)\npalette_1 = ListedColormap(palette_1)\npalette_1\n\n\npalette_2 = sns.diverging_palette(h_neg=200, h_pos=300, s=100, l=50, n=n_colors, as_cmap=False)\npalette_2 = ListedColormap(palette_2)\npalette_2\n\n\n# Combine\nimport numpy as np\npalette_combined = np.vstack((\n    palette_1(np.linspace(0, 1, n_colors)),\n    palette_2(np.linspace(1/n_colors, 1, n_colors - 1))))\nListedColormap(palette_combined, name='GrBlPu')\n\n\n# (5) Spectral (cyclic H, ~constant L)\nListedColormap(sns.color_palette(\"husl\", n_colors=10, as_cmap=False))\n\n\n#cmr.get_sub_cmap('hsv', 0, 1, N=20)\n\n\n### Discrete maps\n#cmr.get_sub_cmap('tab20b', 0, 1, N=20)\n# plt.cm.tab20b\n\n\n### Constructing discrete monochromatic map (hue-grouped truncated discrete map)\nn_groups = 3\nn_bars = 3\ncolors = plt.cm.tab20b((\n    [0,1,2,\n     4,5,6,\n     16,17,18])).reshape(n_groups, n_bars, 4)\nListedColormap(colors.reshape(n_groups * n_bars, 4))\n# ListedColormap(colors[0, :])\n\n\n### Obtain hex/rgb string codes for specified colourmap\n# return_fmt {hex, float=rgb}\n#cmr.take_cmap_colors('cmr.chroma', cmap_range=(0, 1), N=20, return_fmt='float')\n\n\n###\n# [TODO] legend has 1st colour of each group; xaxis has rotated labels of 3*3\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nspecies = (\"Adelie\", \"Chinstrap\", \"Gentoo\")\ndata = {\n    'Bill Depth': (18.35, 18.43, 14.98),\n    'Bill Length': (38.79, 48.83, 47.50),\n    'Flipper Length': (189.95, 195.82, 217.19),\n}\n\nx = np.arange(len(species))  # the label locations\nwidth = 0.2  # the width of the bars\nmultiplier = 0\n\nfig, ax = plt.subplots(layout='constrained')\n\nfor group, (attribute, measurement) in enumerate(data.items()):\n    offset = width * multiplier\n    rects = ax.bar(x + offset, measurement, width, alpha=.85,\n                   label=attribute.lower().capitalize(),\n                   color=colors[:, group])\n    ax.bar_label(rects, padding=3)\n    multiplier += 1\n\nax.grid(axis='y'); ax.legend(ncols=3)\nax.set_ylim(0, 250)\nax.set_xticks(x + width, species)\nax.set_ylabel('Length (mm)')\nax.set_title('Whitepaper plot')\n\nplt.show()\n\n3 Tools for data representation & manipulation\n\n### [def] Utilities for visualizing df and LA operations\n# [REF] Vanderplas; [MB] adapted to handle numpy arrays\nimport numpy as np\nimport numpy.linalg as la\nimport pandas as pd\n\nclass display(object):\n  \"\"\"Display HTML representation of multiple objects\"\"\"\n  template = \"\"\"<div style=\"float: left; padding: 10px;\">\n  <p style='font-family:\"Courier New\", Courier, monospace'>{0}{1}\n  \"\"\"\n  def __init__(self, *args):\n    self.args = args\n\n  def __repr__(self):\n    return '\\n\\n'.join(\n        '\\n' + '\\033[1m' + a + '\\033[0m'\n        + '\\n' + ' ' + repr(np.shape(eval(a))) + ' '\n        + '\\n' + repr(np.round(eval(a), 2))\n        for a in self.args\n    )\n\n\ndef make_df(cols, ind):\n  \"\"\"Quickly make a DataFrame\"\"\"\n  data = {c: [str(c) + str(i) for i in ind]\n          for c in cols}\n  return pd.DataFrame(data, ind)\n\n\n### Examples: Visualizing df operations\ndf1 = make_df('AB', [1, 2])\ndf2 = make_df('AB', [3, 4])\ndisplay('df1', 'df2', 'pd.concat([df1, df2])')\n\n\n### Examples: Visualizing matrix operations\n# Matrix algebra & operations\nX = np.array([[1, 2, 3], [4, 5, 6]])\nY = np.array([[1, 2, 3], [4, 5, 6]])\ndisplay('X', 'Y.T', 'X @ Y.T', 'la.inv(X @ Y.T)', 'la.det(X @ Y.T)')\n\n\n# Outer products\nx = np.array([1, 2])\ny = x + 1\ndisplay('x', 'y', 'np.outer(x, y)')\n\n\n# Eigendecomposition\nX_square = np.vstack((X, np.array([7, 8, 9])))\neig, Evec = la.eig(X_square)\nEig = np.diag(eig)\ndisplay('X_square', 'Eig', 'Evec')\n\n\n# Matrix decomposition\nU, d, V_T = la.svd(X_square)\nD = np.diag(d)\ndisplay('X_square', 'U', 'D', 'V_T')\n\n\n# Forming tensors: From vectors\nx_vec = x[:, np.newaxis]\nx_ten = x_vec[:, :, np.newaxis]\ndisplay('x', 'x_vec', 'x_ten')\n\n\n# Forming tensors: From matrices\nX_3D1 = X[:, :, np.newaxis]\nX_3D2 = X[:, np.newaxis, :]\nX_3D3 = X[np.newaxis, :, :]\ndisplay('X', 'X_3D1', 'X_3D2', 'X_3D3')\n\n\n# Tensor broadcasting: From vectors\nc = np.array([0, 1, 2])\ndisplay('x_vec', 'x_ten',  'c', 'x_ten - c')\n\n\n# Tensor broadcasting: From matrices\ndisplay('X', 'X_3D1', 'c', 'X_3D1 - c')\n\n\n# Reshaping & aggregating: From vectors\ndisplay('x_vec', 'c', 'x_ten - c',\n        '(x_ten - c) ** 2',\n        'np.sum((x_ten - c) ** 2, axis=1)')\n\n\n# Reshaping & aggregating: From tensors\n# [TODO] add tensor dot product for summation over arbitrary dims\ndisplay('X', 'c', 'X_3D1 - c',\n        '(X_3D1 - c) ** 2',\n        '((X_3D1 - c) ** 2).reshape(len(X), 1, -1)',\n        'np.sum(((X_3D1 - c) ** 2).reshape(len(X), 1, -1), axis=1)') # EATS empty 1st dim\n\n4 Tools for matrix visualization\n\n### [def] Tools for visualizing LA operations\nimport numpy as np\nimport numpy.linalg as la\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\n\n# [NOTE] attempted to resolve pyplot matrix rendering issues\n# !pip install array_to_latex\n# import array_to_latex as a2l\n# mpl.rcParams['text.usetex'] = True\n# mpl.rcParams['text.latex.preamble'] = r'\\usepackage{{amsmath}}'\n# plt.rcParams[\"text.latex.preamble\"].join([\n#         r\"\\usepackage{dashbox}\",\n#         r\"\\setmainfont{xcolor}\"])\n# plt.rcParams.update({\"text.usetex\": True})\n\n# # [REF] https://inakleinbottle.com/posts/formatting-matrices-with-python/\ndef format_matrix(matrix, environment=\"pmatrix\", formatter=str):\n    \"\"\"Format a matrix using LaTeX syntax\"\"\"\n\n    if not isinstance(matrix, np.ndarray):\n        try:\n            matrix = np.array(matrix)\n        except Exception:\n            raise TypeError(\"Could not convert to Numpy array\")\n\n    if len(shape := matrix.shape) == 1:\n        matrix = matrix.reshape(1, shape[0])\n    elif len(shape) > 2:\n        raise ValueError(\"Array must be 2 dimensional\")\n\n    body_lines = [\" & \".join(map(formatter, row)) for row in matrix]\n\n    body = \"\\\\\\\\\\n\".join(body_lines)\n    return f\"\"\"\\\\begin{{{environment}}}\n{body}\n\\\\end{{{environment}}}\"\"\"\n\n# [REF] Geron\ndef plot_vector2d(vector2d, origin=[0, 0], **options):\n    return plt.arrow(origin[0], origin[1], vector2d[0], vector2d[1],\n                     linewidth=1, head_width=0.1,\n                     head_length=0.15, length_includes_head=True,\n                     **options)\n\ndef plot_transformation(P_before, P_after, text_before, text_after,\n                        axis=[0, 5, 0, 4], arrows=False, display_mapping=None):\n    if arrows:\n        for vector_before, vector_after in zip(P_before.T, P_after.T):\n            plot_vector2d(vector_before, color=\"blue\", linestyle=\"--\")\n            # plot_vector2d(vector_before, color=\"blue\", linestyle=\"-\")\n            plot_vector2d(vector_after, color=\"red\", linestyle=\"-\")\n\n    if display_mapping is not None:\n      # M_before = r'$\\begin{bmatrix} ' + '\\\\'.join([' & '.join(map(str, row)) for row in P_before]) + '\\end{bmatrix}$'\n      plt.text(axis[1]*.7, axis[3]*.7,\n               f\"{format_matrix(P, environment='bmatrix')}\",\n               bbox=dict(facecolor='white', edgecolor='white', boxstyle='round,pad=0.3'),\n               fontsize=9, color=\"black\")\n\n    plt.gca().add_artist(Polygon(P_before.T, alpha=0.2))\n    plt.gca().add_artist(Polygon(P_after.T, alpha=0.3, color=\"r\"))\n    plt.plot(P_before[0], P_before[1], \"b--\", alpha=0.5)\n    plt.plot(P_after[0], P_after[1], \"r--\", alpha=0.5)\n\n    plt.text(P_before[0].mean(), P_before[1].mean(), text_before,\n             fontsize=12, color=\"blue\")\n    # plt.text(P_before[0].max()*.9, P_before[1].max()*.9, text_before,\n    #          fontsize=12, color=\"blue\")\n    plt.text(P_after[0].mean(), P_after[1].mean(), text_after,\n             fontsize=12, color=\"red\")\n    plt.axis(axis)\n    plt.gca().set_aspect(\"equal\")\n    plt.grid()\n\n\n### Examples: Vectors\nu = np.array([2, 5])\nv = np.array([3, 1])\n\nplt.subplots(figsize=(4,4))\nplot_vector2d(u, color=\"r\")\nplot_vector2d(v, color=\"b\")\nplot_vector2d(v, origin=u, color=\"b\")\nplot_vector2d(u, origin=v, color=\"r\")\nplot_vector2d(u+v, color=\"g\")\n\nplt.axis([0, 6.5, 0, 6.5])\nplt.gca().set_aspect(\"equal\")\nplt.text(0.7, 3, \"u\", color=\"r\", fontsize=12)\nplt.text(4, 3, \"u\", color=\"r\", fontsize=12)\nplt.text(1.8, 0.2, \"v\", color=\"b\", fontsize=12)\nplt.text(3.1, 5.6, \"v\", color=\"b\", fontsize=12)\nplt.text(2.4, 2.5, \"u+v\", color=\"g\", fontsize=12)\nplt.grid()\nplt.show()\n\n\n### Examples: Matrices\n# [def] Helper to get min & max coordinates of start & end matrices\ndef mat_minmax_coords(*matrices, square=False):\n  stacked_matrix = np.hstack(matrices)\n  xy_min = np.min(stacked_matrix, axis=1) * 1.1\n  xy_max = np.max(stacked_matrix, axis=1) * 1.1\n  axis = [xy_min[0], xy_max[0], xy_min[1], xy_max[1]]\n\n  if square:\n    axis_lengths = [xy_max[0] - xy_min[0], xy_max[1] - xy_min[1]]\n    scale_factor = max(axis_lengths)/axis_lengths\n    axis = axis * np.repeat(scale_factor, 2)\n\n  return axis\n\n# Starting matrix\n# P = np.array([[3.0, 4.0, 1.0, 4.6], [0.2, 3.5, 2.0, 0.5]]) # non-convex shape\n# P = np.array([[0.0, 4.0, 6.0, 2.0], [0.0, 0.0, 5.0, 5.0]]) # neater shape\nP = np.array([[0, 0, 1, 1], [0, 1, 1, 0]]) # square\nP = P / 2\nfig, axs = plt.subplots(2,2, figsize=(8,8))\n\n# Scale\nP_rescaled = .6 * P\nplt.sca(axs[0,0]) # [0, 3.5, 0, 3.5]\nplot_transformation(P, P_rescaled, \"$P$\", \"$0.6 P$\",\n                    axis=mat_minmax_coords(P, P_rescaled, square=0),\n                    arrows=False)\n\n# Rotate\nangle30 = 30 * np.pi / 180 # angle in radians\nangle120 = 120 * np.pi / 180 # angle in radians\nV = np.array([\n        [np.cos(angle30), np.sin(angle30)],\n        [np.cos(angle120), np.sin(angle120)]])\nP_rotated = V @ P\nplt.sca(axs[0,1]) # [-1.5, 4, -1.5, 4]\nplot_transformation(P, P_rotated, \"$P$\", \"$V_{rotate} P$\",\n                    axis=mat_minmax_coords(P, P_rotated, square=0),\n                    display_mapping=V)\n\n# Shear\nF_shear = np.array([[1, 1.5], [0, 1]])\nP_sheared = F_shear @ P\nplt.sca(axs[1,0]) # [0, 7, 0, 7]\nplot_transformation(P, P_sheared, \"$P$\", \"$F_{shear} P$\",\n                    axis=mat_minmax_coords(P, P_sheared, square=0))\n\n# Horizontal reflection\nF_reflect = np.array([[1, 0], [0, -1]])\nP_reflected = F_reflect @ P\nplt.sca(axs[1,1]) # [-3, 4, -3, 4]\nplot_transformation(P, P_reflected, \"$P$\", \"$F_{reflect} P$\",\n                    axis=mat_minmax_coords(P, P_reflected, square=0))\n\nplt.show()\n\n\n### Example: SVD\n# M = np.array([[1, 1.5], [0, 1]]) # F_shear\nM = (np.array([[1, 1.5], [0, 1]]))*-1\nSquare = np.array([\n    [0, 0, 1, 1],\n    [0, 1, 1, 0]])\nU, d, V_T = la.svd(M) # 𝜎/Σ_diag, Σ\nD = np.diag(d)\nassert np.any(np.round(U @ D @ V_T, 1) == np.round(M, 1))\n\n\n# SVD\nfig, axs = plt.subplots(1, 3, figsize=(9,6))\naxis = mat_minmax_coords(\n    Square, V_T @ Square, D @ V_T @ Square, U @ D @ V_T @ Square, square=True)\n\nplt.sca(axs[0]) # no need to explicitly plot with axs handle for custom plotter\nplot_transformation(Square, V_T @ Square, \"$Square$\", r\"$V^T \\cdot Square$\",\n                    axis=axis)\nplt.title('$(V^T) \\cdot Square$')\n\nplt.sca(axs[1])\nplot_transformation(V_T @ Square, D @ V_T @ Square,\n                    r\"$V^T \\cdot Square$\", r\"$\\Sigma \\cdot V^T \\cdot Square$\",\n                    axis=axis)\nplt.title('$(\\Sigma \\cdot V^T) \\cdot Square$')\n\nplt.sca(axs[2])\nplot_transformation(D @ V_T @ Square, U @ D @ V_T @ Square,\n                    r\"$\\Sigma \\cdot V^T \\cdot Square$\",\n                    r\"$U \\cdot \\Sigma \\cdot V^T \\cdot Square$\",\n                    axis=axis)\nplt.title('$(U \\cdot \\Sigma \\cdot V^T) \\cdot Square$')\n\n# plt.suptitle('SVD')\n# plt.tight_layout()\nplt.show()\n\n5 Preliminary tests\n\n### [def] Generate B/W image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(19680801) # seed random number generator\n\ndef generate_stripe_image(size, stripe_nr, vertical = True):\n  \"\"\"Generate random (black) square image w/ `stripe_nr` (white) stripes\"\"\"\n\n  img = np.zeros((size, size, 1), dtype = \"uint8\") # init size^2 img of 0's\n  for i in range(0, stripe_nr):\n      x, y = np.random.randint(0, size, 2) # gen 2 rand int btwn 0 & size\n      l = np.int(np.random.randint(y, size, 1)) # rand int btwn y & size\n      if (vertical): # set to True by default\n          img[y:l, x, 0] = 255 # add vertical line from y:l in rand col x\n      else:\n          img[x, y:l, 0] = 255 # horizontal line in row x\n  return img\n\n\n### [fig] Example: Plot image\n# plot example image for reference\nimg = generate_stripe_image(50, 10, vertical = True)\nplt.imshow(img[:, :, 0], cmap = 'gray')\n\n\n### Generate training and validation sets\nn_img = 1000\n# n_stripes = 10\n# img_size = 50\n\n# initialize empty arrays to store training/validation sets\nX_train = X_val = np.empty([n_img, 50, 50, 1])\nvert = True # initially generate vertical images\nn_vert = .5*n_img\n\n## generate images\nfor i in range(np.shape(X_train)[0]):\n    # start generating horizontal images half-way through loop\n    if i == n_vert - 1:\n      vert = False\n\n    X_train[i, :, :, :] = generate_stripe_image(50, 10, vert)\n    X_val[i, :, :, :] = generate_stripe_image(50, 10, vert)\n\n## (standard) normalize training and validation sets (based on training data)\nX_mean = np.mean(X_train, axis = 0)\nX_std = np.std(X_train, axis = 0) + 1e-7 # to deal w/ potential sd's of 0\nX_train = (X_train - X_mean) / X_std\nX_val = (X_val - X_mean) / X_std\n\n\n### Store true class labels (0 = vert. stripes; 1 = horiz.)\nY_train = np.zeros(500)\nY_train = Y_val = np.append(Y_train, np.ones(500))\n\n\n# Basics\n\n\nfig, ax = plt.subplots()\nax.plot([1, 2, 3], [4, 5, 6], label='Example line 1')\nax.plot([1, 1.5, 2], [4, 5, 6], label='Example line 2')\nax.scatter([1, 2, 3], [4, 5, 6])\nax.scatter([1, 1.5, 2], [4, 5, 6])\n\nax.grid(); ax.legend()\nax.set_xlim(.75,3.25); #ax.set_ylim(0,3)\nax.set_xticks([1,1.5,2,2.5,3]); #ax.set_yticks([1,2,3])\nax.set_xlabel('$x$'); ax.set_ylabel('$y$')\nax.set_title('Lines & scatter')\n\nplt.show()\n\n\n# [TODO] compute bar heights from randomly generated averages & add error bar\n# fig, ax = plt.subplots(figsize=(6/1.2,4*1.2))\nfig, ax = plt.subplots(layout='constrained')\nax.bar([1, 2, 3], [4, 5, 6], label='Example bar 1', width=.3, alpha=.85)\nax.bar([1.5, 2.5], [4, 6], label='Example bar 2', width=.3, alpha=.85)\n\nax.grid(which='major', axis='y'); ax.legend()\nax.set_xticks([1,2,3])\nax.set_xlabel('$x$'); ax.set_ylabel('$y$')\nax.set_title('Bars')\n\n# plt.tight_layout()\nplt.show()\n\n\n### Pivot table for numeric column aggregated wrt 2 cats (group/level)\n# [TODO] move to data exploration, above grouping/joins\nimport seaborn as sns\nimport pandas as pd\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Specify the categorical variables for levels and groups\nlevel_variable = \"sex\"\ngroup_variable = \"day\"\n\n# Create a pivot table to structure the data\npivot_table = tips.pivot_table(values=\"total_bill\", index=level_variable, columns=group_variable, aggfunc=\"mean\")\n\npivot_table\n\n\n### Pivot table for num column w/ non-aggregated values wrt 2 cats (group/level)\n# For quickly plotting group comparisons\n# [TODO] move to data exploration, above grouping/joins\nimport seaborn as sns\nimport pandas as pd\n\n# Load the tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Specify the categorical variables for levels and groups\nlevel_variable = \"sex\"\ngroup_variable = \"day\"\n\n# Group by the specified variables and collect total_bill values as arrays\ngrouped_data = tips.groupby([level_variable, group_variable])[\"total_bill\"].apply(list).unstack()\n\n# Display the resulting DataFrame\ngrouped_data\n\n\n### Stat + CI for num column w/ non-aggregated values wrt 2 cats (group/level)\n# For quickly obtaining stats and err for group comparisons\n# [TODO] move to data exploration, above grouping/joins\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# Assume grouped_data is a DataFrame with multi-index (group, level) and values as arrays\n# Replace 'total_bill' with the actual column name containing arrays\ngrouped_data = tips.groupby(['sex', 'day'])['total_bill'].apply(np.array)\n\n# Function to calculate mean and confidence interval\ndef calculate_mean_ci(data):\n    # Convert non-numeric values to NaN and drop them\n    numeric_data = pd.Series(data).apply(pd.to_numeric, errors='coerce').dropna()\n\n    if len(numeric_data) > 0:\n        mean = np.mean(numeric_data)\n        ci = stats.t.interval(0.95, len(numeric_data) - 1, loc=np.mean(numeric_data), scale=stats.sem(numeric_data))\n        return mean, ci\n    else:\n        return np.nan, (np.nan, np.nan)\n\n# Apply the function to each group\nresult_list = []\n\nfor (group, level), data in grouped_data.items():\n    mean, ci = calculate_mean_ci(data)\n    result_list.append({\n        'Group': group,\n        'Level': level,\n        'Mean': mean,\n        'Confidence Interval': ci\n    })\n\n# Display the list of means and confidence intervals\nfor result in result_list:\n    print(result)\n\n\n# [TODO] add option to plot sigbars for within group comparisons instead of btwn\n# [TODO] add option to plot sigbars for betwn group comps of particular level\n# [TODO] check accuracy of stats\n# [TODO] clean up\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy import stats\nfrom itertools import combinations\n\n# Specify the number of groups and bars per group\nN_GROUPS = 4\nN_LEVELS = 3\n\n# Generate or use actual data for averages and confidence intervals\ndata = np.random.normal(loc=5, scale=1.0, size=(N_GROUPS, N_LEVELS, 10))  # Adjusted the number of samples\n\n# Modify a couple of bar values to demonstrate significant differences\ndata[0, 0, :] += 2.5  # Increase the first bar in the first group\ndata[2, 2, :] -= 2.5  # Decrease the third bar in the third group\n\n# Calculate averages and confidence intervals\naverages = np.mean(data, axis=2)\nconf_intervals = np.zeros_like(averages, dtype=float)\n\nfor group_idx in range(N_GROUPS):\n    for level_idx in range(N_LEVELS):\n        interval = stats.t.interval(0.95, len(data[group_idx, level_idx]) - 1,\n                                    loc=np.mean(data[group_idx, level_idx]),\n                                    scale=stats.sem(data[group_idx, level_idx]))\n        conf_intervals[group_idx, level_idx] = np.abs(interval[1] - averages[group_idx, level_idx])  # Use upper bound\n\n# Plot grouped bars with confidence intervals\nwidth = 0.2\ncolors = plt.cm.viridis(np.linspace(0, 1, N_LEVELS))\nline_thickness = 0.7  # Adjust the line thickness\nstagger_amount = 0.8  # Increased the stagger amount\n\nfig, ax = plt.subplots()\n\nfor level_idx in range(N_LEVELS):\n    bars = ax.bar(np.arange(N_GROUPS) + level_idx * width - (width * (N_LEVELS - 1) / 2),\n                  averages[:, level_idx],\n                  yerr=conf_intervals[:, level_idx],\n                  width=width,\n                  alpha=0.85,\n                  capsize=3,\n                  color=colors[level_idx],\n                  label=f'Level {level_idx + 1}',\n                  error_kw={'elinewidth': line_thickness})  # Set the line thickness for error bars\n\n# Set labels and title\nax.set_xlabel('Groups')\nax.set_ylabel('Values')\nax.set_title('Grouped Bars with Confidence Intervals')\n\n# Set x-axis ticks and labels\ngroup_labels = [f'Group {i}' for i in range(1, N_GROUPS + 1)]\nax.set_xticks(np.arange(N_GROUPS))\nax.set_xticklabels(group_labels)\n\n# Add legend\nax.legend(title='Levels', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Add staggered significance bars and asterisks for select between-group comparisons\nsignificance_level = 0.05\nstagger_index = 0\n\nfor comb in combinations(range(N_GROUPS), 2):x\n    group1_center = ax.get_xticks()[comb[0]]\n    group2_center = ax.get_xticks()[comb[1]]\n\n    t_stat, p_value = stats.ttest_ind(data[comb[0], :, :].flatten(), data[comb[1], :, :].flatten())\n    if p_value < significance_level:\n        tallest_bar_height = np.max(averages) + np.max(conf_intervals) + 0.5\n        significance_height = tallest_bar_height + np.max(conf_intervals) * 0.07 + stagger_index * stagger_amount  # Adjust the stagger amount\n\n        # Plot staggered horizontal lines aligned with the midpoints of the compared groups\n        ax.plot([group1_center, group2_center],\n                [significance_height] * 2, color='black', lw=line_thickness)\n\n        # Plot asterisks aligned with the center of the significance bars\n        asterisks = '*' * sum([p_value < alpha for alpha in [0.01, 0.001, 0.0001]])\n        ax.text((group1_center + group2_center) / 2, significance_height,\n                asterisks, ha='center', va='bottom', fontsize=10)\n\n        stagger_index += 1  # Increment the index for staggered bars\n\n        # Print significant comparisons, t-test results, and sample sizes\n        sample_size1 = len(data[comb[0], :, :].flatten())\n        sample_size2 = len(data[comb[1], :, :].flatten())\n        print(f'Significant comparison between {group_labels[comb[0]]} and {group_labels[comb[1]]}: '\n              f'p-value = {p_value}, \\nt-statistic = {t_stat}, '\n              f'Sample Size: {group_labels[comb[0]]} = {sample_size1}, {group_labels[comb[1]]} = {sample_size2}\\n')\n\n# Show the plot\nax.grid(axis='y')\nplt.tight_layout()\nplt.show()\n\n\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib.ticker import FormatStrFormatter\n\ndata = sns.load_dataset(\"iris\")\ndata_ = data.rename(columns=lambda name: name.replace('_', ' ').capitalize())\n\nfig, axs = plt.subplots()\naxs = scatter_matrix(data_, ax=axs, diagonal='hist', alpha=.75,\n                     hist_kwds=dict(alpha=.75),\n)\n\n# Iterate through the axes to set the y-axis formatter\nfor ax in axs.flatten():\n    # ax.yaxis.set_major_formatter(FormatStrFormatter('%05.2f'))\n    ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n    ax.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n    ax.tick_params(axis='both', labelsize=7)\n    ax.xaxis.label.set_size(8)\n    ax.yaxis.label.set_size(8)\n    # ax.grid()\n\nfig.suptitle(\"Scatter matrix\")\nplt.show()\n\n\n# [TODO] Add example where axes flipped so text horizontal with bars\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nspecies = (\"Adelie\", \"Chinstrap\", \"Gentoo\")\npenguin_means = {\n    'Bill Depth': (18.35, 18.43, 14.98),\n    'Bill Length': (38.79, 48.83, 47.50),\n    'Flipper Length': (189.95, 195.82, 217.19),\n}\n\nx = np.arange(len(species))  # the label locations\nwidth = 0.2  # the width of the bars\nspacing = 0.009 # the space between bars within groups\nmultiplier = 0\n\nwith sns.color_palette('viridis', n_colors=3, as_cmap=False):\n# color=ListedColormap(plt.cm.tab20b((np.arange(0,9))))\n  fig, ax = plt.subplots(layout='constrained')\n\nfor attribute, measurement in penguin_means.items():\n    offset = (width + spacing) * multiplier\n    rects = ax.bar(x + offset, measurement, width, alpha=.85,\n                   label=attribute.lower().capitalize())\n    ax.bar_label(rects, padding=3)\n    multiplier += 1\n\nax.grid(axis='y'); ax.legend(ncols=3)\nax.set_ylim(0, 250)\n# ax.set_xticks(x + width, species)\nax.set_xticks(x + ((width + spacing) * (multiplier - 1) / 2), species)\nax.set_ylabel('Length (mm)')\nax.set_title('Whitepaper plot')\n\nplt.show()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nspecies = (\"Adelie\", \"Chinstrap\", \"Gentoo\")\npenguin_means = {\n    'Bill Depth': (18.35, 18.43, 14.98),\n    'Bill Length': (38.79, 48.83, 47.50),\n    'Flipper Length': (189.95, 195.82, 217.19),\n}\n\nx = np.arange(len(species))  # the label locations\nwidth = 0.2  # the width of the bars\nspacing = 0.0  # the space between bars within groups\nmultiplier = 0\n\nwith sns.color_palette('viridis', n_colors=3, as_cmap=False):\n    fig, ax = plt.subplots(layout='constrained', figsize=(6, 3.5))\n\n    for attribute, measurement in penguin_means.items():\n        offset = (width + spacing) * multiplier\n        rects = ax.barh(x + offset, measurement, height=width, alpha=.85,\n                        label=attribute.lower().capitalize())\n        ax.bar_label(rects, padding=3)\n        multiplier += 1\n\n    ax.grid(axis='x')\n    ax.legend(ncols=3)\n    ax.set_xlim(0, 250)\n    ax.set_yticks(x + ((width + spacing) * (multiplier - 1) / 2))\n    ax.set_yticklabels(species)\n    ax.set_xlabel('Length (mm)')\n    ax.set_title('Transposed whitepaper plot')\n\nplt.show()\n\n6 Grid\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# At least one dimension must be > 1\nN_ROWS = 2\nN_COLS = 3\n\n# Sample data for each subplot in groups of 2\nx = np.linspace(0, 2 * np.pi, 100)\nfunctions = []\ntitles = []\nfor i in range(N_ROWS*N_COLS * 2):\n  functions.append(lambda x: np.sin((i + 1) * x))\n  functions.append(lambda x: np.cos((i + 1) * x))\n\n  if (i % 2 == 0): titles.append(f'$a = {int(i/2 + 1)}$')\n\n# Create a 2x2 subplots grid using a for loop\nfig, axs = plt.subplots(N_ROWS, N_COLS, sharey=True)\n\n# Flatten the axs array for easier iteration\naxs = axs.flatten()\n\n# Loop through subplots and plot data\nfor i, ax in enumerate(axs):\n    y1 = functions[i * 2](x)\n    y2 = functions[(i * 2) + 1](x)\n    ax.plot(x, y1)\n    ax.plot(x, y2)\n\n    # To add legend to each frame\n    # ax.legend(labels=[f'$sin({i + 1}x)$', f'$cos({i + 1}x)$'], loc=2)\n\n    # Set title of each frame and label only rows and columns\n    ax.set_title(titles[i])\n    if (i > (N_ROWS - 1) * N_COLS - 1): ax.set_xlabel('$x$')\n    if i % N_COLS == 0: ax.set_ylabel('$y$')\n\n    ax.grid()\n\n# Add legend for groups\nfig.legend(labels=['$sin(ax)$', '$cos(ax)$'], bbox_to_anchor=(1, .89), loc=2)\n\n# Use tight layout (default) to prevent clipping of titles & center\nfig.supylabel('$y$')\nfig.supxlabel('$x$', x=.58)\nplt.suptitle('Subplot grid', x=.58)\n\nplt.show()\n\n7 Seaborn extensions for statistical plotting\n\nimport seaborn as sns\nfrom matplotlib.ticker import StrMethodFormatter\n\ndata = sns.load_dataset(\"iris\")\ndata_ = data.rename(columns=lambda name: name.replace('_', ' ').capitalize())\n\nhist_kws    = dict(edgecolor='black', linewidth=.5) # bins\nkde_kws     = dict(linewidth=.5)\nscatter_kws = dict(alpha=.6, edgecolor='black', linewidth=.1) # s\nline_kws    = dict(linewidth=1)\nreg_kws     = dict(scatter_kws=scatter_kws, line_kws=line_kws)\n\nwith sns.axes_style('whitegrid'), sns.color_palette('viridis'):\n  # plt.figure(figsize=(6, 6))\n  ax = sns.pairplot(data_, vars=['Petal width', 'Petal length', 'Sepal length'],\n                    hue='Species',\n                    kind='reg', diag_kind='kde',\n                    plot_kws=reg_kws, diag_kws=kde_kws,\n  )\n\nplt.suptitle('Grouped pairplot')\nplt.tight_layout()\nplt.show()\n# ax.axes.set_major_formatter(StrMethodFormatter(f'{{x:.2f}}'))\n# sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, .75))\n\n\n# Grouped boxplots\n# [TODO] add sigstars to significant within-group differences\ntips = sns.load_dataset('tips')\n\n# with sns.axes_style('whitegrid'), sns.color_palette('viridis', n_colors=2):\n# with sns.color_palette('viridis', n_colors=2):\nplt.figure()\nax = sns.boxplot(data=tips, x='day', y='total_bill', hue='sex',\n                #  dodge=1,\n                 gap=.2, width=.6,\n                 saturation=.7,\n                 fliersize=3.75,\n                #  whis=.5,\n                #  linecolor='black', linewidth=.5,\n)\n\n# plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nax.grid(axis='y')\nax.legend(ncol=2, loc='best')\nplt.xlabel('Category'); plt.ylabel('Value')\nplt.title('Grouped boxplots')\n\nplt.tight_layout()\nplt.show()\n\n\ntitanic = sns.load_dataset(\"titanic\")\n\nwith sns.axes_style('whitegrid'):\n  plt.figure()\n  ax = sns.FacetGrid(titanic, col=\"survived\", row=\"sex\")\n  ax = ax.map(plt.hist, \"age\", alpha=.85, bins=10)\n\nax.set_xlabels('Age')\nplt.suptitle('Bifaceted wildcard plot')\n# plt.tight_layout()\nplt.show()\n\n8 Additional seaborn plots\n[TODO]\nMissing\n1. overlapping densities\n2. overlapping scatter\n3. jointplot\n4. √ bars with error bar\n5. timeseries w/ error bar\n\ntips = sns.load_dataset('tips')\n# hist_kws = dict(edgecolor='black', linewidth=.5) # bins\nline_kws = dict(linewidth=1)\nkde_kws  = dict() # linewidth=.5\n\nwith sns.axes_style('whitegrid'):\n  plt.figure()\n  ax = sns.histplot(data=tips, x='total_bill', #hue='sex',\n                    kde=True, element='bars', stat='count',\n                    line_kws=line_kws, kde_kws=kde_kws,\n  )\n\nplt.xlim(-10, 60)\nplt.xlabel('Value')\nplt.title('Histogram with KDE')\n\nplt.tight_layout()\nplt.show()\n\n\n# Correlation matrix and heatmap\n# [TODO] Make these colours nicer\niris = sns.load_dataset('iris')\niris_ = iris.rename(columns=lambda name: name.replace('_', ' ').capitalize())\ncorrelation_matrix = iris_.corr(numeric_only=True)\n\n#with sns.plotting_context(font_scale=.1):\nplt.figure()\nax = sns.heatmap(correlation_matrix,\n                 annot=True, cbar=True, cmap=plt.cm.PiYG,\n                 square=True,\n                 linewidths=0,\n                 #linecolor='black', edgecolor='black',\n)\n\nplt.title('Heatmap')\nplt.show()\n\n9 Math\n[TODO]\nMissing\n1. 3D scatter\n2. 2D cartesian\n3. 2surface\n4. curve\n5. 22D vector field\n6. 2D contour plot\nML plots:\n7. [TODO] 3D data + PCs -> transformed 2D\n8. [TODO] clean up generative (HoML), cluster/DR comparison\nFunctionalize:\n12. [TODO] functionalize ML plots (args: fit model; X/y train/test (test optional); DR/manifold method)\n13. [TODO] functionalize ML comparisons (args: model builds, labs, dataset lists; DR/man meth; dataset kwargs)\n[Multi] For class/clust functions:\n14. [TODO] add support for multi-class via appropriate custom colour generator calls\n15. [TODO] add support for multi-feature class/clust via appropriate DR/manifold method (class DR, clust man)\n[Prep] For class/clust/manifold functions:\n16. [TODO] include appropriate prep for each feature (num: scale, cat: enc, ts: smooth, word: embed, sent: tfidf)\n[Datasets] For ML comparison * class/clust functions:\n17. [TODO] include std set of datasets (blob/circ/lin) * std args (blob cov kw: sphere/elip/ndiag, deg over, rand)\nLater, for model-specific validations:\n- fix standard dataset & DR/manifold method & prep\n\niris = sns.load_dataset('iris')\n\nwith sns.axes_style('whitegrid'):\n  plt.figure()\n  sns.kdeplot(data=iris, x='sepal_length', y='sepal_width',\n              cmap='Blues', fill=True, levels=5,\n  )\n\nplt.title('Joint contour plot')\nplt.xlabel('Feature 1'); plt.ylabel('Feature 2')\nplt.tight_layout()\nplt.show()\n\n10 Analysis\n[TODO]\ndef math_plotter(\n  f: lambda {\n    function(x, y), curve(t), surface(u, v), field(x, y, z)},\n  type: string {\n    '2D', '3D', 'contour', 'curve', 'surface', 'field'},\n  coordinates: string = 'rect' {\n    'rect', 'radial', 'cylindrical', 'spherical'},\n  bounds: List(\n    {x, t, u}: List(lo: float, hi: float) = [-10., 10.],\n    {y, v}: List(lo: float, hi: float) = [-10., 10.],\n    {z}: List(lo: float, hi: float) = [-10., 10.])\n ) -> plot: mpl_object {\n    f_plot: mpl_subplot,\n    gradient_plot: mpl_subplot = None,\n    integral_{plot, text} (explicit 2D graph if '2D' type given and closed form exists; else numerical estimate wrt some specified bounds):\n    {mpl_subplot, string} = None}\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef f(x, y):\n  return np.sin(x) * np.cos(y)\n\n# Create a grid of points & corresponding smaller grid for a neater quiver plot\nn = 100\nX, Y = np.meshgrid(np.linspace(-5, 5, n), np.linspace(-5, 5, n))\n# X_quiv, Y_quiv = np.meshgrid(np.linspace(-5, 5, int(n/10)),\n#                              np.linspace(-5, 5, int(n/10)))\n\n# Compute the derivative of the surface\nZ = f(X, Y)\n# Z_quiv = f(X_quiv, Y_quiv)\ndx, dy = np.gradient(Z)\n# dx, dy = np.gradient(Z_quiv)\n\n# Plot the surface and its derivative\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis')\nax.quiver(X, Y, Z, dx, dy, np.zeros_like(dx),\n          arrow_length_ratio=.3, color='red')\n\n# Set the axes labels\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\n# Show the plot\nplt.show()\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.integrate as integrate\n\n# Calculate \\int^\\infty_0 e^{-x} dx\n# invexp = lambda x: np.exp(-x)\n# integrate.quad(invexp, 0, np.inf)\n# function = lambda x, y: x**2 + y**2\n# integrate.quad(function, 0, np.inf)\n\ndef integral(x, y):\n    # return integrate.quad(lambda t: np.sqrt((x**2 + y**2 - 2*x*y*np.cos(np.pi*t*(np.sqrt(1/x**3) - np.sqrt(1/y**3))))/(x**3*y**3)), 0,  np.sqrt(x**3*y**3))[0]\n    return integrate.quad(lambda t: t*x**2 + t*y**2, 0, x**2 + y**2)[0]\n\n# X = np.arange(0.1, 5, 0.1)\n# Y = np.arange(0.1, 5, 0.1)\nX = np.arange(-10, 10, .1)\nY = np.arange(-10, 10, .1)\nX, Y = np.meshgrid(X, Y)\nZ = np.vectorize(integral)(X, Y)\n\nfig = plt.figure()\nax = plt.axes(projection=\"3d\")\nax.plot_wireframe(X, Y, Z, color='green')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\n\nax = plt.axes(projection='3d')\nax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n                norm=plt.Normalize(np.nanmin(Z), np.nanmax(Z)),\n                cmap='winter', edgecolor='none')\n\n# ax.set_xlim(-10, 10); ax.set_ylim(-10, 10); ax.set_zlim(-10, 10)\nplt.show()\n\n11 Stats\n[TODO]\nPlot common 2/3D dists alongside estimated:\n- mean/variance\n- probability wrt some specified bounds\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\nrandom_seed=1000\n\n# Distribution params (3 covariance options, shared mean)\ncov_val = [-0.8, 0, 0.8]\nmean = np.array([0,0])\n\n# Container for density functions for further analysis\npdf_list = []\n\n## Plot densities\nax = plt.figure(figsize=(8,6))\n\n# iterate over different covariance values\nfor idx, val in enumerate(cov_val):\n\n    # Initialize the covariance matrix\n    cov = np.array([[1, val], [val, 1]])\n\n    # Generate Gaussian bivariate dist with given mean and covariance matrix\n    distr = multivariate_normal(cov = cov, mean = mean,\n                                seed = random_seed)\n\n    # Generate a meshgrid complacent with the 3-sigma boundary\n    mean_1, mean_2 = mean[0], mean[1]\n    sigma_1, sigma_2 = cov[0,0], cov[1,1]\n\n    x = np.linspace(-3*sigma_1, 3*sigma_1, num=100)\n    y = np.linspace(-3*sigma_2, 3*sigma_2, num=100)\n    X, Y = np.meshgrid(x,y)\n\n    # Evaluate density for each point in the meshgrid\n    pdf = np.zeros(X.shape)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            pdf[i,j] = distr.pdf([X[i,j], Y[i,j]])\n    pdf_list.append(pdf)\n\n    # Plot density function values\n    ax = plt.subplot(1, 3, idx + 1, projection = '3d')\n    ax.plot_surface(X, Y, pdf, cmap = 'viridis')\n\n    ax.axes.zaxis.set_ticks([])\n    # plt.xlabel(\"$x_1$\")\n    # plt.ylabel(\"$x_2$\")\n    #plt.title(f'Covariance between x1 and x2 = {val}')\n\nplt.tight_layout()\nplt.show()\n\n## Plot contour maps\nax = plt.figure(figsize=(8,6))\nfor idx, val in enumerate(pdf_list):\n    plt.subplot(2, 3, idx + 1, aspect=1)\n    plt.contourf(X, Y, val, cmap='viridis')\n\n    plt.xlabel(\"$x_1$\")\n    plt.ylabel(\"$x_2$\")\n    #plt.title(f'Covariance between x1 and x2 = {cov_val[idx]}')\n\nplt.tight_layout()\nplt.show()\n\n12 Classifier deep-dive\n\n### Groundwork (data, preprocess, decompose, model, and datashape inspection\nimport numpy as np\nfrom timeit import default_timer as timer\n\nfrom sklearn import datasets\nfrom sklearn import linear_model, naive_bayes, svm, tree, ensemble\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.decomposition import PCA, KernelPCA\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# Mutable parameters and obtain colourmap\nn_features = 7\nn_classes = 5\n\n# higher resolutions reduce pixelation and thin out decision boundary (300)\nresolution = 300\n\n# higher values crowd plot (100)\nn_samples=1000\n\ncmap = cmr.get_sub_cmap('gnuplot2', .1, .8, N=n_classes)\ncolor_list = [cmap(i)[:3] for i in range(cmap.N)]\n# ListedColormap(color_list)\n\n# \n# (1) Make data\nX, y = datasets.make_classification(\n    n_samples=n_samples, n_features=n_features,\n    n_informative=n_features, n_redundant=0,\n    n_clusters_per_class=1, n_classes=n_classes,\n    random_state=42,\n)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2,\n                                                    random_state=42)\n\n# (2) Preprocess, decompose and project feature space for later plotting\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X_train)\n# dr_model = PCA(n_components=2)\ndr_model = KernelPCA(n_components=2, fit_inverse_transform=True)\nX2D = dr_model.fit_transform(scaler.fit_transform(X_train))\n\n# (3) Build, fit\n# model = linear_model.LogisticRegression()\nmodel = naive_bayes.GaussianNB()\n# model = svm.SVC(kernel='rbf', C=6, probability=True, random_state=42)\n# model = tree.DecisionTreeClassifier(max_depth=100)\n# model = ensemble.RandomForestClassifier(n_estimators=100, max_depth=100, criterion='gini')\n# model = MLPClassifier(\n#     hidden_layer_sizes=np.size(X_train, 1) * 1,\n#     activation='relu', solver='adam',\n#     batch_size=round(.01 * len(X)),\n#     alpha=.001, momentum=.9\n# )\n\nt_start = timer()\nmodel.fit(X_scaled, y_train)\nt_end = timer()\nt_elapsed = round(t_end - t_start, 4)\n\n# Get projected scatter boundaries for plotting\nx_min = X2D[:, 0].min() - 0.1 * (X2D[:, 0].max() - X2D[:, 0].min())\nx_max = X2D[:, 0].max() + 0.1 * (X2D[:, 0].max() - X2D[:, 0].min())\ny_min = X2D[:, 1].min() - 0.1 * (X2D[:, 1].max() - X2D[:, 1].min())\ny_max = X2D[:, 1].max() + 0.1 * (X2D[:, 1].max() - X2D[:, 1].min())\nxrg = [x_min, x_max]; yrg = [y_min, y_max]\naxis = [x_min, x_max, y_min, y_max]\n\n# (4a) Define projected inference grid wrt original feature space (inversion)\nxx, yy = np.meshgrid(\n    np.arange(x_min, x_max, 1. * (x_max - x_min) / resolution),\n    np.arange(y_min, y_max, 1. * (y_max - y_min) / resolution)\n)\nX_grid = np.c_[xx.ravel(), yy.ravel()]\nX_grid_inverse = scaler.inverse_transform(dr_model.inverse_transform(X_grid))\n# assert np.all(list(zip(np.ravel(xx), np.ravel(yy))) == X_grid)\n\n# (4b) Get probabilistic and deterministic predictions over inference grid\nZpp = model.predict_proba(X_grid_inverse)\nZp = model.predict(X_grid_inverse)\n# Z = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n\n# (P1) Obtain decision boundary line within projected feature space\n# as [...]\nZp_grid = Zp.reshape(xx.shape)\nZb = np.zeros(xx.shape)\nZb[:-1, :] = np.maximum((Zp_grid[:-1, :] != Zp_grid[1:, :]), Zb[:-1, :])\nZb[1:, :] = np.maximum((Zp_grid[:-1, :] != Zp_grid[1:, :]), Zb[1:, :])\nZb[:, :-1] = np.maximum((Zp_grid[:, :-1] != Zp_grid[:, 1:]), Zb[:, :-1])\nZb[:, 1:] = np.maximum((Zp_grid[:, :-1] != Zp_grid[:, 1:]), Zb[:, 1:])\n\n# (P2) Obtain graded probability surface within projected feature space\n# as [(n_samples, n_classes) probability surface] * [(n_classes, 3) color vec]\n# yielding (Zp_grid size) grid with RGB color vector values\ncolors = np.array(color_list[0:n_classes])\nzz = np.dot(Zpp, colors)\nzz_r = zz.reshape(xx.shape[0], xx.shape[1], colors.shape[1])\n\n# \n# (1-2) Attributes, Targets > Features > Decomposed features >\n# (4a) Inference grid, Ravelled into features, Inverted to og feature space >\n# (4b) Probabilistic + Deterministic predictions,\n# (P1) Unravelled into grid for plotting > Decision boundary line >\n# (P2) Colour-graded probability surface\ndisplay('X', 'y', 'X_scaled', 'X2D',\n        'xx', 'yy', 'X_grid', 'X_grid_inverse',\n        'Zpp', 'Zp',\n        'Zp_grid', '(Zp_grid[:-1, :])', '(Zp_grid[1:, :])', '(Zb[:-1, :])', 'Zb',\n        'colors', 'zz', 'zz_r',\n        )\n\n\n### Plotting: illustrative plot of probability surface\n# [TODO] implement cross-validation here instead of a single train/test split\n# [TODO] define search grids for each model and complexity knobs for val\n# [TODO] pipe prep & model & plot grid/rand search results below as heat or lol\n# [TODO] plot training/validation curve & learning curve below on 1 line\n# [TODO] incorporate all models from sklearn classifier comparison plot\n# [TODO] repeat for clustering, manifold\n\nimport matplotlib.patches as mpatches\n\naxis_lim = [min(xrg[0], yrg[0]), max(xrg[1], yrg[1])]\naxis_equal = axis_lim + axis_lim\n\n# Prep test data identically to training data, evaluate\nX_scaled_test = scaler.fit_transform(X_test)\ny_pred = model.predict(X_scaled_test)\naccuracy_val = accuracy_score(y_pred=y_pred, y_true=y_test)\n\ny_pred_train = model.predict(X_scaled)\naccuracy_train = accuracy_score(y_pred=y_pred_train, y_true=y_train)\n\nX2D_test = dr_model.fit_transform(X_scaled_test)\n\n# \nfig, ax = plt.subplots(1, 1, figsize=(6,6))\n\n# Boundary line\nax.imshow(Zb, origin='lower', interpolation=None, cmap='Greys',\n          alpha=1.0, extent=axis_equal)\n\n# Probability surface\nax.imshow(zz_r, origin='lower', interpolation=None,\n          alpha=.7, extent=axis_equal)\n\n# Scatter\nax.scatter(X2D[:, 0], X2D[:, 1], c=[colors[i, :] for i in y_train],\n          #  linewidths=.5, edgecolors='k',\n           )\n\n# \n# Add legend\ncolors_bar = []\nfor v1 in colors[:n_classes, :]:\n  v1 = list(v1)\n  v1.append(.7)\n  colors_bar.append(v1)\n\n# create a patch (proxy artist) for every color\npatches = [mpatches.Patch(color=colors_bar[i],\n                          label=\"Class {k}\".format(k=i))\n          for i in range(n_classes)]\n# put those patched as legend-handles into the legend\nplt.legend(handles=patches, bbox_to_anchor=(1.05, 1),\n            loc=2, borderaxespad=0., framealpha=0.5)\n\nplt.grid()\n# ax.set_aspect('equal', adjustable='box')\n\nax.set_xlim(axis_lim)\nax.set_ylim(axis_lim)\n\nif dr_model is None:\n    plt.xlabel('Raw axis $x_1$')\n    plt.ylabel('Raw axis $x_2$')\nelse:\n    plt.xlabel('Dimension reduced axis 1')\n    plt.ylabel('Dimension reduced axis 2')\n\nax.set_title(f\"{str(model)[:]}\\ndecision surface\", size=10)\nax.text(\n    axis_lim[1] - 0.3, axis_lim[0] + 0.3,\n    f\"Training accuracy: {accuracy_train}, Validation accuracy: {accuracy_val}\\n\\\n    Training time: {t_elapsed}s\",\n    size=9,\n    horizontalalignment=\"right\",\n    bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\"),\n    # transform=ax.transAxes,\n)\nplt.suptitle(\n    f\"{n_classes} classes by {n_features} features\\n\\\n    {len(X_train)} training samples, {len(X_test)} testing samples\",\n    x=1 - .4, y=0 + .025,\n    # ha='center', va='bottom',\n)\n# Training accuracy: {accuracy_train}, Validation accuracy: {accuracy_val}\\n\\\n# Training time: {t_elapsed} s\",\n\nplt.show()\n\n# confusion_matrix(y_pred=y_pred, y_true=y_test)\nconfusion_matrix(y_pred=y_pred_train, y_true=y_train)\n\n13 ML\n\n# 2-class classifier plot with decision boundary\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom sklearn.datasets import make_classification\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\n# Make data and preprocess\nn_classes = 2\nX, y = make_classification(n_samples=500, n_features=2,\n                           n_informative=2, n_redundant=0,\n                           n_clusters_per_class=1, n_classes=n_classes,\n)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Build classifier, fit, and predict\nsvm = SVC(kernel='rbf', random_state=42, probability=True)\nsvm.fit(X_scaled, y)\nxx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\nZ = svm.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\nZ = Z.reshape(xx.shape)\n\n# Plot scatter with decision surface\nwith sns.axes_style('whitegrid'):\n  plt.figure()\n  plt.contourf(xx, yy, Z, cmap='PiYG', alpha=0.8) # cmap='coolwarm'\n  plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y,\n              cmap=ListedColormap([\"#b30065\", \"#178000\"]),\n              # cmap=cmr.get_sub_cmap('PiYG', .1, .9, N=n_classes),\n              # cmap='viridis',\n  )\n\nplt.grid()\nplt.xlim(-3, 3); plt.ylim(-3, 3)\nplt.xlabel('Feature 1'); plt.ylabel('Feature 2')\nplt.title('2-class classifier with scatter and decision surface')\n\nplt.show()\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import datasets, svm\nfrom sklearn.inspection import DecisionBoundaryDisplay\n\n# Data\niris = datasets.load_iris()\nX = iris.data[:, :2]  # only take first two features\nY = iris.target # 3 classes\n\n# Build classifier & fit\nclf = svm.SVC(kernel='rbf')\nclf.fit(X, Y)\n\n# Plot model\n# h = 0.02  # step size in the mesh\nwith sns.axes_style('whitegrid'):\n  plt.figure()\n  ax = plt.gca()\n  DecisionBoundaryDisplay.from_estimator(\n      clf, X, ax=ax,\n      cmap=plt.cm.viridis_r, grid_resolution=100,\n      response_method=\"predict\", plot_method=\"pcolormesh\", shading=\"auto\",\n      alpha=.85\n  )\n\n# Plot scatter\nplt.scatter(X[:, 0], X[:, 1], c=Y, s=20,\n            cmap=plt.cm.viridis_r, edgecolors=\"k\", linewidths=1, alpha=.85,\n)\n\nplt.xlabel('Feature 1'); plt.ylabel('Feature 2')\nplt.title(\"3-class classifier with scatter and decision boundary\")\nplt.axis(\"tight\")\nplt.show()\n\n\n# Generative kernels and graded densities\nfrom sklearn.datasets import make_blobs\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\n\n# Get data & preprocess\nX, y = make_blobs(n_samples=500, centers=3, random_state=42, cluster_std=3.0)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Model & fit\ngmm = GaussianMixture(n_components=3, random_state=42)\ngmm.fit(X_scaled)\n\n# Build grid & predict\nh = 0.02\nx_min, x_max = X_scaled[:, 0].min() - 1, X_scaled[:, 0].max() + 1\ny_min, y_max = X_scaled[:, 1].min() - 1, X_scaled[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot model & scatter\nwith sns.axes_style('whitegrid'):\n  plt.figure()\n  plt.contourf(xx, yy, Z, levels=20, alpha=0.8, cmap='viridis')\n  scatter = sns.scatterplot(\n      x=X_scaled[:, 0], y=X_scaled[:, 1], hue=y, s=20,\n      palette='pastel', edgecolor='black', alpha=.9,\n  )\n  # Plot centers\n  plt.scatter(\n      gmm.means_[:, 0], gmm.means_[:, 1],\n      # color='red', marker='o', s=30, alpha=.85,\n      color='white', marker='o', s=75,\n      edgecolor='black', linewidths=.5,\n  )\n\nplt.xlabel('Feature 1'); plt.ylabel('Feature 2')\nplt.xlim(x_min, x_max); plt.ylim(y_min, y_max)\nplt.title('3-class clustering with scatter and densities')\n\nhandles, labels = scatter.get_legend_handles_labels()\nplt.legend(handles, ['Cluster ' + item for item in labels],\n           loc='best', #bbox_to_anchor=(1, 0.5),\n           frameon=1,\n)\nplt.show()\n\n14 Model comparison routines (dataset x model/hyperparameter)\n\n# Code source: Gaël Varoquaux\n#              Andreas Müller\n# Modified for documentation by Jaques Grobler\n# License: BSD 3 clause\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.datasets import make_circles, make_classification, make_moons\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.inspection import DecisionBoundaryDisplay\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n## Make models\nnames = [\n    # \"Nearest Neighbors\",\n    # \"Linear SVM\",\n    \"RBF SVM\",\n    # \"Gaussian Process\",\n    \"Decision Tree\",\n    \"Random Forest\",\n    \"Neural Net\",\n    # \"AdaBoost\",\n    \"Naive Bayes\",\n    # \"QDA\",\n]\n\nclassifiers = [\n    # KNeighborsClassifier(3),\n    # SVC(kernel=\"linear\", C=0.025, random_state=42),\n    SVC(gamma=2, C=1, random_state=42),\n    # GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n    DecisionTreeClassifier(max_depth=5, random_state=42),\n    RandomForestClassifier(\n        max_depth=5, n_estimators=10, max_features=1, random_state=42\n    ),\n    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n    # AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n    GaussianNB(),\n    # QuadraticDiscriminantAnalysis(),\n]\n\n## Make datasets\nX, y = make_classification(\n    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n)\nrng = np.random.RandomState(2)\nX += 2 * rng.uniform(size=X.shape)\nlinearly_separable = (X, y)\n\ndatasets = [\n    make_moons(noise=0.3, random_state=0),\n    make_circles(noise=0.2, factor=0.5, random_state=1),\n    linearly_separable,\n]\n\n## Plot\nfigure = plt.figure(figsize=(8, 6)) # figure = plt.figure(figsize=(27, 9))\n\n# iterate over datasets\ni = 1\nfor ds_cnt, ds in enumerate(datasets):\n    ## Split into training and test part\n    X, y = ds\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.4, random_state=42\n    )\n\n    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n\n    ## Plot the dataset\n    cm = plt.cm.PiYG # cm = plt.cm.RdBu\n    cm_bright = ListedColormap([\"#b30065\", \"#178000\"]) # cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n    if ds_cnt == 0:\n        ax.set_title(\"Input data\")\n\n    # Plot the training points & testing points (latter slightly faded)\n    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n               cmap=cm_bright, edgecolors=\"k\",\n    )\n    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test,\n               cmap=cm_bright, edgecolors=\"k\", alpha=0.6,\n    )\n\n    # Plotting adjustments\n    ax.set_xlim(x_min, x_max)\n    ax.set_ylim(y_min, y_max)\n    ax.set_xticks(())\n    ax.set_yticks(())\n    i += 1\n\n    # iterate over classifiers\n    for name, clf in zip(names, classifiers):\n        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n\n        ## Preprocess, fit, and plot fit\n        clf = make_pipeline(StandardScaler(), clf)\n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)\n        DecisionBoundaryDisplay.from_estimator(\n            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n        )\n\n        # Plot the training points & testing points (latter slightly faded)\n        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train,\n                   cmap=cm_bright, edgecolors=\"k\",\n        )\n        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test,\n                   cmap=cm_bright, edgecolors=\"k\", alpha=0.6,\n        )\n\n        # Plotting adjustments & annotation\n        ax.set_xlim(x_min, x_max)\n        ax.set_ylim(y_min, y_max)\n        ax.set_xticks(())\n        ax.set_yticks(())\n        if ds_cnt == 0:\n            ax.set_title(name)\n\n        ax.text(\n            x_max - 0.3,\n            y_min + 0.3,\n            (\"%.2f\" % score).lstrip(\"0\"),\n            size=12,\n            horizontalalignment=\"right\",\n            bbox=dict(boxstyle=\"round\", alpha=0.8, facecolor=\"white\"),\n            # transform=ax.transAxes,\n        )\n        i += 1\n\nfigure.suptitle('Classifier comparison')\n# figure.supxlabel('x')\n# figure.supylabel('y')\nplt.tight_layout()\nplt.show()\n\n\n# [TODO] polish this\nimport time\nimport warnings\nfrom itertools import cycle, islice\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn import cluster, datasets, mixture\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.preprocessing import StandardScaler\n\n# ============\n# Generate datasets. We choose the size big enough to see the scalability\n# of the algorithms, but not too big to avoid too long running times\n# ============\nn_samples = 500\nseed = 30\nnoisy_circles = datasets.make_circles(\n    n_samples=n_samples, factor=0.5, noise=0.05, random_state=seed\n)\nnoisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=seed)\nblobs = datasets.make_blobs(n_samples=n_samples, random_state=seed)\nrng = np.random.RandomState(seed)\nno_structure = rng.rand(n_samples, 2), None\n\n# Anisotropicly distributed data\nrandom_state = 170\nX, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\ntransformation = [[0.6, -0.6], [-0.4, 0.8]]\nX_aniso = np.dot(X, transformation)\naniso = (X_aniso, y)\n\n# blobs with varied variances\nvaried = datasets.make_blobs(\n    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n)\n\n# ============\n# Set up cluster parameters\n# ============\nplt.figure(figsize=(12, 8)) # (9 * 2 + 3, 13)\nplt.subplots_adjust(\n    left=0.02, right=0.98, bottom=0.001, top=0.95, wspace=0.05, hspace=0.01\n)\n\nplot_num = 1\n\ndefault_base = {\n    \"quantile\": 0.3,\n    \"eps\": 0.3,\n    \"damping\": 0.9,\n    \"preference\": -200,\n    \"n_neighbors\": 3,\n    \"n_clusters\": 3,\n    \"min_samples\": 7,\n    \"xi\": 0.05,\n    \"min_cluster_size\": 0.1,\n    \"allow_single_cluster\": True,\n    \"hdbscan_min_cluster_size\": 15,\n    \"hdbscan_min_samples\": 3,\n    \"random_state\": 42,\n}\n\ndatasets = [\n    (\n        noisy_circles,\n        {\n            \"damping\": 0.77,\n            \"preference\": -240,\n            \"quantile\": 0.2,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.08,\n        },\n    ),\n    (\n        noisy_moons,\n        {\n            \"damping\": 0.75,\n            \"preference\": -220,\n            \"n_clusters\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n        },\n    ),\n    (\n        varied,\n        {\n            \"eps\": 0.18,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.01,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (\n        aniso,\n        {\n            \"eps\": 0.15,\n            \"n_neighbors\": 2,\n            \"min_samples\": 7,\n            \"xi\": 0.1,\n            \"min_cluster_size\": 0.2,\n        },\n    ),\n    (blobs, {\"min_samples\": 7, \"xi\": 0.1, \"min_cluster_size\": 0.2}),\n    (no_structure, {}),\n]\n\nfor i_dataset, (dataset, algo_params) in enumerate(datasets):\n    # update parameters with dataset-specific values\n    params = default_base.copy()\n    params.update(algo_params)\n\n    X, y = dataset\n\n    # normalize dataset for easier parameter selection\n    X = StandardScaler().fit_transform(X)\n\n    # estimate bandwidth for mean shift\n    bandwidth = cluster.estimate_bandwidth(X, quantile=params[\"quantile\"])\n\n    # connectivity matrix for structured Ward\n    connectivity = kneighbors_graph(\n        X, n_neighbors=params[\"n_neighbors\"], include_self=False\n    )\n    # make connectivity symmetric\n    connectivity = 0.5 * (connectivity + connectivity.T)\n\n    # ============\n    # Create cluster objects\n    # ============\n    ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n    two_means = cluster.MiniBatchKMeans(\n        n_clusters=params[\"n_clusters\"],\n        random_state=params[\"random_state\"],\n    )\n    ward = cluster.AgglomerativeClustering(\n        n_clusters=params[\"n_clusters\"], linkage=\"ward\", connectivity=connectivity\n    )\n    spectral = cluster.SpectralClustering(\n        n_clusters=params[\"n_clusters\"],\n        eigen_solver=\"arpack\",\n        affinity=\"nearest_neighbors\",\n        random_state=params[\"random_state\"],\n    )\n    dbscan = cluster.DBSCAN(eps=params[\"eps\"])\n    # hdbscan = cluster.HDBSCAN(\n    #     min_samples=params[\"hdbscan_min_samples\"],\n    #     min_cluster_size=params[\"hdbscan_min_cluster_size\"],\n    #     allow_single_cluster=params[\"allow_single_cluster\"],\n    # )\n    optics = cluster.OPTICS(\n        min_samples=params[\"min_samples\"],\n        xi=params[\"xi\"],\n        min_cluster_size=params[\"min_cluster_size\"],\n    )\n    affinity_propagation = cluster.AffinityPropagation(\n        damping=params[\"damping\"],\n        preference=params[\"preference\"],\n        random_state=params[\"random_state\"],\n    )\n    average_linkage = cluster.AgglomerativeClustering(\n        linkage=\"average\",\n        metric=\"cityblock\",\n        n_clusters=params[\"n_clusters\"],\n        connectivity=connectivity,\n    )\n    birch = cluster.Birch(n_clusters=params[\"n_clusters\"])\n    gmm = mixture.GaussianMixture(\n        n_components=params[\"n_clusters\"],\n        covariance_type=\"full\",\n        random_state=params[\"random_state\"],\n    )\n\n    clustering_algorithms = (\n        (\"MiniBatch\\nKMeans\", two_means),\n        (\"Affinity\\nPropagation\", affinity_propagation),\n        (\"MeanShift\", ms),\n        (\"Spectral\\nClustering\", spectral),\n        (\"Ward\", ward),\n        (\"Agglomerative\\nClustering\", average_linkage),\n        (\"DBSCAN\", dbscan),\n        # (\"HDBSCAN\", hdbscan),\n        (\"OPTICS\", optics),\n        (\"BIRCH\", birch),\n        (\"Gaussian\\nMixture\", gmm),\n    )\n\n    for name, algorithm in clustering_algorithms:\n        t0 = time.time()\n\n        # catch warnings related to kneighbors_graph\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"the number of connected components of the \"\n                + \"connectivity matrix is [0-9]{1,2}\"\n                + \" > 1. Completing it to avoid stopping the tree early.\",\n                category=UserWarning,\n            )\n            warnings.filterwarnings(\n                \"ignore\",\n                message=\"Graph is not fully connected, spectral embedding\"\n                + \" may not work as expected.\",\n                category=UserWarning,\n            )\n            algorithm.fit(X)\n\n        t1 = time.time()\n        if hasattr(algorithm, \"labels_\"):\n            y_pred = algorithm.labels_.astype(int)\n        else:\n            y_pred = algorithm.predict(X)\n\n        plt.subplot(len(datasets), len(clustering_algorithms), plot_num)\n        if i_dataset == 0:\n            plt.title(name) # size=18\n\n        colors = np.array(\n            list(\n                islice(\n                    cycle(\n                        [\n                            \"#377eb8\",\n                            \"#ff7f00\",\n                            \"#4daf4a\",\n                            \"#f781bf\",\n                            \"#a65628\",\n                            \"#984ea3\",\n                            \"#999999\",\n                            \"#e41a1c\",\n                            \"#dede00\",\n                        ]\n                    ),\n                    int(max(y_pred) + 1),\n                )\n            )\n        )\n        # add black color for outliers (if any)\n        colors = np.append(colors, [\"#000000\"])\n        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_pred])\n\n        plt.xlim(-2.5, 2.5)\n        plt.ylim(-2.5, 2.5)\n        plt.xticks(())\n        plt.yticks(())\n        plt.text(\n            0.99,\n            0.01,\n            (\"%.2fs\" % (t1 - t0)).lstrip(\"0\"),\n            transform=plt.gca().transAxes,\n            # size=15,\n            horizontalalignment=\"right\",\n        )\n        plot_num += 1\n\nplt.show()\n\n\n# [TODO] polish this\n# Author: Jaques Grobler <jaques.grobler@inria.fr>\n# License: BSD 3 clause\n\nfrom time import time\n\nimport matplotlib.pyplot as plt\n\n# Unused but required import for doing 3d projections with matplotlib < 3.2\nimport mpl_toolkits.mplot3d  # noqa: F401\nimport numpy as np\nfrom matplotlib.ticker import NullFormatter\n\nfrom sklearn import manifold\nfrom sklearn.utils import check_random_state\n\n# Variables for manifold learning.\nn_neighbors = 10\nn_samples = 1000\n\n# Create our sphere.\nrandom_state = check_random_state(0)\np = random_state.rand(n_samples) * (2 * np.pi - 0.55)\nt = random_state.rand(n_samples) * np.pi\n\n# Sever the poles from the sphere.\nindices = (t < (np.pi - (np.pi / 8))) & (t > ((np.pi / 8)))\ncolors = p[indices]\nx, y, z = (\n    np.sin(t[indices]) * np.cos(p[indices]),\n    np.sin(t[indices]) * np.sin(p[indices]),\n    np.cos(t[indices]),\n)\n\n# Plot our dataset.\nfig = plt.figure(figsize=(10, 6)) # (15, 8)\nplt.suptitle(\n    \"Manifold Learning with %i points, %i neighbors\" % (1000, n_neighbors),\n) # fontsize=14\n\nax = fig.add_subplot(251, projection=\"3d\")\nax.scatter(x, y, z, c=p[indices], cmap=plt.cm.rainbow)\nax.view_init(40, -10)\n\nsphere_data = np.array([x, y, z]).T\n\n# Perform Locally Linear Embedding Manifold learning\nmethods = [\"standard\", \"ltsa\", \"hessian\", \"modified\"]\nlabels = [\"LLE\", \"LTSA\", \"Hessian LLE\", \"Modified LLE\"]\n\nfor i, method in enumerate(methods):\n    t0 = time()\n    trans_data = (\n        manifold.LocallyLinearEmbedding(\n            n_neighbors=n_neighbors, n_components=2, method=method, random_state=42\n        )\n        .fit_transform(sphere_data)\n        .T\n    )\n    t1 = time()\n    print(\"%s: %.2g sec\" % (methods[i], t1 - t0))\n\n    ax = fig.add_subplot(252 + i)\n    plt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\n    plt.title(\"%s (%.2g sec)\" % (labels[i], t1 - t0))\n    ax.xaxis.set_major_formatter(NullFormatter())\n    ax.yaxis.set_major_formatter(NullFormatter())\n    plt.axis(\"tight\")\n\n# Perform Isomap Manifold learning.\nt0 = time()\ntrans_data = (\n    manifold.Isomap(n_neighbors=n_neighbors, n_components=2)\n    .fit_transform(sphere_data)\n    .T\n)\nt1 = time()\nprint(\"%s: %.2g sec\" % (\"ISO\", t1 - t0))\n\nax = fig.add_subplot(257)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(\"%s (%.2g sec)\" % (\"Isomap\", t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\"tight\")\n\n# Perform Multi-dimensional scaling.\nt0 = time()\nmds = manifold.MDS(2, max_iter=100, n_init=1, random_state=42)\ntrans_data = mds.fit_transform(sphere_data).T\nt1 = time()\nprint(\"MDS: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(258)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(\"MDS (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\"tight\")\n\n# Perform Spectral Embedding.\nt0 = time()\nse = manifold.SpectralEmbedding(\n    n_components=2, n_neighbors=n_neighbors, random_state=42\n)\ntrans_data = se.fit_transform(sphere_data).T\nt1 = time()\nprint(\"Spectral Embedding: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(259)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(\"Spectral Embedding (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\"tight\")\n\n# Perform t-distributed stochastic neighbor embedding.\nt0 = time()\ntsne = manifold.TSNE(n_components=2, random_state=0)\ntrans_data = tsne.fit_transform(sphere_data).T\nt1 = time()\nprint(\"t-SNE: %.2g sec\" % (t1 - t0))\n\nax = fig.add_subplot(2, 5, 10)\nplt.scatter(trans_data[0], trans_data[1], c=colors, cmap=plt.cm.rainbow)\nplt.title(\"t-SNE (%.2g sec)\" % (t1 - t0))\nax.xaxis.set_major_formatter(NullFormatter())\nax.yaxis.set_major_formatter(NullFormatter())\nplt.axis(\"tight\")\n\nplt.show()\n\n\n\n\n",
    "preview": "roadmap_gallery/2024-03-15-plotting-temp/../../images/attribute_histogram_plots.png",
    "last_modified": "2024-03-16T00:30:13-04:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 1800
  },
  {
    "path": "roadmap_gallery/2024-03-15-writing-temp/",
    "title": "Technical reports with R Markdown & Bookdown",
    "description": "I provide markdown snippets for a variety of typesetting elements that can \nbe produced with `R Markdown` and the `Bookdown` package. I focus on\nthe elements that I find most useful for typesetting and annotating math.",
    "author": [
      {
        "name": "Matthew Bain",
        "url": {}
      }
    ],
    "date": "2024-03-16",
    "categories": [
      "reference"
    ],
    "contents": "\n\nContents\n1 Key idea environments\n2 Math elements\n3 Assorted math examples\n3.1 Braces\n3.2 Equations (+ brackets/arr/mat; int/summ/pmat)\n\n4 Aligned math environments\n4.1 Numbering specific steps\n4.2 Passage numbering w/ align-embedded split\n\n5 Math environments\n5.1 Passage numbering\n5.2 Individual lines numbering\n\n6 Code blocks\n7 Images & figures\n8 asides\n9 In-text references\n9.1 Core materials\n9.2 Web materials\n9.3 Cross-references to in-text elements\n\n10 Embedded media\n11 Miscellania\n11.1 HTML/CSS elements\n11.2 Text formatting\n\n\n1 Key idea environments\n\\[\\begin{align}\ng(X_{n}) &= g(\\theta)+g'({\\tilde{\\theta}})(X_{n}-\\theta) \\notag \\\\\n\\sqrt{n}[g(X_{n})-g(\\theta)] &= g'\\left({\\tilde{\\theta}}\\right)\n  \\sqrt{n}[X_{n}-\\theta ] \\tag{1}\n\\end{align}\\]\n\\[\\begin{equation}\n\\begin{split}\n\\mathrm{Var}(\\hat{\\beta}) & =\\mathrm{Var}((X'X)^{-1}X'y)\\\\\n& =(X'X)^{-1}X'\\mathrm{Var}(y)((X'X)^{-1}X')'\\\\\n& =(X'X)^{-1}X'\\mathrm{Var}(y)X(X'X)^{-1}\\\\\n& =(X'X)^{-1}X'\\sigma^{2}IX(X'X)^{-1}\\\\\n& =(X'X)^{-1}\\sigma^{2}\n\\end{split}\n\\tag{2}\n\\end{equation}\\]\n\nTheorem 1  (Pythagorean theorem) For a right triangle, if \\(c\\) denotes the length of the hypotenuse\nand \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have\n\\[a^2 + b^2 = c^2\\]\n\n\nProof (Pythagorean theorem). Let \\(x, y\\) be …\nThus, \\[a^2 + b^2 = c^2,\\]\nas desired.\n\\[\n\\tag*{$\\square$}\n\\]\n\n2 Math elements\n\\[\n\\underbrace{\\ln \\left( \\frac{5}{6} \\right)}_{\\simeq -0.1823}\n< \\overbrace{\\exp \\left(\\frac{1}{2} \\right)}^{\\simeq 1.6487}\n\\]\n\nThe bookdown package provides HTML support for numbered math equations.\nHowever, unlike in pure LaTeX, in bookdown we must manually assign labels\nto every line that should have a number.\nI do so for significant passages, steps, and equations.\n\\[\n\\begin{equation}\n  x = a_0 + \\cfrac{1}{a_1\n          + \\cfrac{1}{a_2\n          + \\cfrac{1}{a_3 + \\cfrac{1}{a_4} } } }\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\frac{\n    \\begin{array}[b]{r}\n      \\left( x_1 x_2 \\right)\\\\\n      \\times \\left( x'_1 x'_2 \\right)\n    \\end{array}\n  }{\n    \\left( y_1y_2y_3y_4 \\right)\n  }\n\\end{equation}\n\\]\n\\[\n\\int_0^\\infty \\mathrm{e}^{-x}\\,\\mathrm{d}x\n\\]\n\\[\n\\sum_{\\substack{\n   0<i<m \\\\\n   0<j<n\n  }}\nP(i,j)\n\\]\n\\[\n\\int\\limits_a^b\n\\]\n\\[\n( a ), [ b ], \\{ c \\}, | d |, \\| e \\|,\n\\langle f \\rangle, \\lfloor g \\rfloor,\n\\lceil h \\rceil, \\ulcorner i \\urcorner,\n/ j \\backslash\n\\]\n\\[\n( \\big( \\Big( \\bigg( \\Bigg(\n\\]\n\\[\n\\begin{matrix}\n  a & b & c \\\\\n  d & e & f \\\\\n  g & h & i\n\\end{matrix}\n\\]\n\\[\n% hello there\nA_{m,n} =\n\\begin{pmatrix}\n  a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n  a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  a_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{pmatrix}\n\\]\n\\[\n\\left(\\!\n    \\begin{array}{c}\n      n \\\\\n      r\n    \\end{array}\n  \\!\\right) = \\frac{n!}{r!(n-r)!}\n\\]\n3 Assorted math examples\n3.1 Braces\n\\[\n\\underbrace{\\ln \\left( \\frac{5}{6} \\right)}_{\\simeq -0.1823}\n< \\overbrace{\\exp \\left(\\frac{1}{2} \\right)}^{\\simeq 1.6487}\n\\]\n3.2 Equations (+ brackets/arr/mat; int/summ/pmat)\n\\[\n\\begin{equation}\n  x = a_0 + \\cfrac{1}{a_1\n          + \\cfrac{1}{a_2\n          + \\cfrac{1}{a_3 + \\cfrac{1}{a_4} } } }\n\\end{equation}\n\\]\n\\[\n\\begin{equation}\n\\frac{\n    \\begin{array}[b]{r}\n      \\left( x_1 x_2 \\right)\\\\\n      \\times \\left( x'_1 x'_2 \\right)\n    \\end{array}\n  }{\n    \\left( y_1y_2y_3y_4 \\right)\n  }\n\\end{equation}\n\\]\n\\[\n\\int_0^\\infty \\mathrm{e}^{-x}\\,\\mathrm{d}x\n\\]\n\\[\n\\sum_{\\substack{\n   0<i<m \\\\\n   0<j<n\n  }}\nP(i,j)\n\\]\n\\[\n\\int\\limits_a^b\n\\]\n\\[\n( a ), [ b ], \\{ c \\}, | d |, \\| e \\|,\n\\langle f \\rangle, \\lfloor g \\rfloor,\n\\lceil h \\rceil, \\ulcorner i \\urcorner,\n/ j \\backslash\n\\]\n\\[\n( \\big( \\Big( \\bigg( \\Bigg(\n\\]\n\\[\n\\begin{matrix}\n  a & b & c \\\\\n  d & e & f \\\\\n  g & h & i\n\\end{matrix}\n\\]\n\\[\n\\left[\\begin{matrix}\n  a & b & c \\\\\n  d & e & f \\\\\n  g & h & i\n\\end{matrix}\\right]\\left[\\begin{matrix}\n  \\cos(t) \\\\\n  \\sin(t) \\\\\n  t\n\\end{matrix}\\right], ~~ t = 0…1\n\\]\n\\[\n% hello there\nA_{m,n} =\n\\begin{pmatrix}\n  a_{1,1} & a_{1,2} & \\cdots & a_{1,n} \\\\\n  a_{2,1} & a_{2,2} & \\cdots & a_{2,n} \\\\\n  \\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n  a_{m,1} & a_{m,2} & \\cdots & a_{m,n}\n\\end{pmatrix}\n\\]\n\\[\n\\left(\\!\n    \\begin{array}{c}\n      n \\\\\n      r\n    \\end{array}\n  \\!\\right) = \\frac{n!}{r!(n-r)!}\n\\]\n\\[\\begin{equation}\n  \\label{eq:1}\n  \\phi \\left( x; \\mu, \\sigma \\right) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( -\n  \\frac{\\left(x - \\mu\\right)^2}{2 \\sigma^2} \\right)\n\\end{equation}\\]\n\\[X = \\begin{bmatrix}1 & x_{1}\\\\\n1 & x_{2}\\\\\n1 & x_{3}\n\\end{bmatrix}\\]\nreference equation \\(\\ref{eq:1}\\)\n4 Aligned math environments\n4.1 Numbering specific steps\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  \\tag{3}\n\\end{equation}\\]\n\\[\\begin{equation}\n  f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k}\n  \\tag{4}\n\\end{equation}\\]\nSee (3)\n\\[\\begin{align}\n    \\sum_{i=1}^{n} \\left( X_i - \\overline{X} \\right )\n    & = \\sum_{i=1}^{n}X_i - \\sum_{i=1}^{n} \\overline{X}\n    &&& \\text{(comment 1)} \\\\\n  \n    & = \\sum_{i=1}^{n} X_i - n \\overline{X}\n    &&& \\text{(comment 2)} \\label{pythag-res} \\\\\n    & = \\sum_{i=1}^{n}X_i - \\sum_{i=1}^{n}X_i \\notag \\\\\n    & = 0\n\\end{align}\\]\nNotes\nSuppress line number w/ \\notag\nComment w/ && \\text{(comment)}\nTag w/ \\label{crossref-name}\nCross-referenced lines must be numbered (so do not mix \\notag and\n\\label)\n4.2 Passage numbering w/ align-embedded split\n\\[\n\\begin{align}\n\\begin{split}\n    a & = b \\\\\n    & = c \\\\\n      & = d \\\\\n    & =e\n\\end{split}\n\\end{align}\n\\]\n5 Math environments\n5.1 Passage numbering\n5.2 Individual lines numbering\n6 Code blocks\n\n## Display the Fibonacci sequence up to n-th term\n\nnterms = int(input(\"How many terms? \"))\n\n# First two terms\nn1, n2 = 0, 1\ncount = 0\n\n# Check if the number of terms is valid\nif nterms <= 0:\n   print(\"Please enter a positive integer\")\n\n# If there is only one term, return n1\nelif nterms == 1:\n   print(\"Fibonacci sequence upto\",nterms,\":\")\n   print(n1)\n\n# Generate fibonacci sequence\nelse:\n   print(\"Fibonacci sequence:\")\n   while count < nterms:\n       print(n1)\n       nth = n1 + n2\n       \n       # update values\n       n1 = n2\n       n2 = nth\n       count += 1\n\n7 Images & figures\n8 asides\nThis content will appear in the gutter of the article.\n9 In-text references\n9.1 Core materials\n(James et al. 2013)\n(Axler, 2019)1\n9.2 Web materials\nThis result can be shown using simulations in\nmatplotlib2.\n\n9.3 Cross-references to in-text elements\nA reference to a header: section 1\nA reference to a math equation: ??\nA reference to a math environment: \nA reference to a figure or image: \n10 Embedded media\n11 Miscellania\n11.1 HTML/CSS elements\n11.2 Text formatting\n\nstrikethrough\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013. An Introduction to Statistical Learning. Vol. 112. Springer.\n\n\nAxler, S. (1997). Linear algebra done right. Springer Science &\nBusiness Media.↩︎\nhttps://codegolf.stackexchange.com/questions/255822/replace-0s-in-a-string-with-their-consecutive-counts\n\n↩︎\n",
    "preview": {},
    "last_modified": "2024-03-16T00:30:06-04:00",
    "input_file": {}
  }
]
